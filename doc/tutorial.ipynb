{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Short Introduction to NeuRec\n",
    "\n",
    "This example aims to describe the building blocks of NeuRec.\n",
    "\n",
    "Following this example, researchers can fast implement their idea and conduct experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "- numpy>=1.17\n",
    "- scipy>=1.3.1\n",
    "- pandas>=0.17\n",
    "- reckit==0.2.0\n",
    "- tensorflow==1.14.0 or pytorch==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurator\n",
    "\n",
    "Read configuration\n",
    "\n",
    "The class `Configurator` is designed to read arguments from ini-style configuration files and/or parse arguments from command line.\n",
    "The arguments can convert to `int`, `float`, `bool`, `list` and `None` automatically.\n",
    "\n",
    "The format of arguments in command line is \"--arg_name arg_value\":\n",
    "```bash\n",
    "python main.py --model MF --num_thread 8 --metric [\"Recall\", \"NDCG\"]\n",
    "```\n",
    "\n",
    "Using `Configurator.add_config()` to read ini-style configuration files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reckit import Configurator\n",
    "\n",
    "config = Configurator()\n",
    "config.add_config(\"Preprocess.ini\", section=\"Preprocess\")\n",
    "# config.parse_cmd()  # Parse the arguments from command line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument `section` will be activated only if there are more than one sections in configuration file, i.e. if there is only one section and whatever the name is, the arguments will be read from it.\n",
    "\n",
    "**Note**, the arguments from command line have the highest priority than that from configuration file.\n",
    "That is, if there are same argument name in configuration file and command line, the value in the former will be overwritten by that in the latter, whenever the command line is phased before or after reading ini files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor\n",
    "\n",
    "This process is not necessary.\n",
    "If your dataset has already preprocessed, you can directly go to [Dataset](#Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\nfiltering items...\nfiltering users...\nremapping user IDs...\nremapping item IDs...\nsplitting data by ratio...\nsaving data to disk...\n2020-09-07 09:15:45.636: \ncolumns = UIRT\nfilename = dataset/ml-100k.rating\nsep = \t\nitem_min = 5\nuser_min = 5\nremap_user_id = True\nremap_item_id = True\nsplit_by = ratio\ntrain = 0.7\nvalid = 0.0\ntest = 0.3\nby_time = False\n2020-09-07 09:15:45.636: Data statistic:\n2020-09-07 09:15:45.637: The number of users: 943\n2020-09-07 09:15:45.637: The number of items: 1349\n2020-09-07 09:15:45.638: The number of ratings: 99287\n2020-09-07 09:15:45.638: Average actions of users: 105.29\n2020-09-07 09:15:45.639: Average actions of items: 73.60\n2020-09-07 09:15:45.639: The sparsity of the dataset: 92.195075%\n"
     ]
    }
   ],
   "source": [
    "from reckit import Preprocessor\n",
    "\n",
    "data = Preprocessor()\n",
    "data.load_data(config.filename, sep=config.separator, columns=config.file_column)\n",
    "if config.drop_duplicates is True:\n",
    "    data.drop_duplicates(keep=\"first\")  # drop duplicates except for the first or last occurrence\n",
    "\n",
    "data.filter_data(user_min=5, item_min=5)  # filter users and items with a few interactions\n",
    "if config.remap_id is True:\n",
    "    data.remap_data_id()  # convert user and item IDs to integers, start from 0\n",
    "\n",
    "if config.splitter == \"leave_out\":\n",
    "    data.split_data_by_leave_out(valid=config.valid, test=config.test,\n",
    "                                 by_time=config.by_time)\n",
    "elif config.splitter == \"ratio\":\n",
    "    data.split_data_by_ratio(train=config.train, valid=config.valid,\n",
    "                             test=config.test, by_time=config.by_time)\n",
    "\n",
    "data.save_data()  # save the preprocessed data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "config = Configurator()\n",
    "config.add_config(\"NeuRec.ini\", section=\"NeuRec\")  # read basic settings\n",
    "# config.parse_cmd()\n",
    "\n",
    "model_cfg = os.path.join(\"conf\", \"MF.ini\")  # model cfg path\n",
    "config.add_config(model_cfg, section=\"hyperparameters\", used_as_summary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prefix name of data files is same as the data_dir, and the suffix/extension names are 'train', 'test', 'user2id', 'item2id'.\n",
    "\n",
    "Directory structure:\n",
    "\n",
    "    data_dir\n",
    "        ├── data_dir.train      // training data\n",
    "        ├── data_dir.valid      // validation data, optional\n",
    "        ├── data_dir.test       // test data\n",
    "        ├── data_dir.user2id    // user to id, optional\n",
    "        ├── data_dir.item2id    // item to id, optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset statistics:\nName: ml-100k_ratio_u5_i5\nThe number of users: 943\nThe number of items: 1349\nThe number of ratings: 99287\nAverage actions of users: 105.29\nAverage actions of items: 73.60\nThe sparsity of the dataset: 92.195075%\n\nThe number of training: 69918\nThe number of validation: 0\nThe number of testing: 29369\n"
     ]
    }
   ],
   "source": [
    "from data import Dataset\n",
    "dataset = Dataset(config.data_dir, config.sep, config.file_column)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger\n",
    "\n",
    "This class can show a message on standard output and write it into the file named `filename` simultaneously.\n",
    "This is convenient for observing and saving training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-07 09:15:45.762: Dataset statistics:\nName: ml-100k_ratio_u5_i5\nThe number of users: 943\nThe number of items: 1349\nThe number of ratings: 99287\nAverage actions of users: 105.29\nAverage actions of items: 73.60\nThe sparsity of the dataset: 92.195075%\n\nThe number of training: 69918\nThe number of validation: 0\nThe number of testing: 29369\n2020-09-07 09:15:45.763: NeuRec:[NeuRec]:\nrecommender=MF\nplatform=pytorch\ndata_dir=dataset/ml-100k_ratio_u5_i5\nfile_column=UIRT\nsep='\\t'\ngpu_id=0\ngpu_mem=0.99\nmetric=[\"Recall\", \"NDCG\"]\ntop_k=[10,20]\ntest_thread=4\ntest_batch_size=64\nseed=2020\n\nMF:[hyperparameters]:\nlr=0.001\nreg=0.001\nembedding_size=64\nbatch_size=1024\nepochs=500\nis_pairwise=True\nloss_func=bpr\nparam_init=normal\n"
     ]
    }
   ],
   "source": [
    "from reckit import Logger\n",
    "import time\n",
    "# create logger filename\n",
    "data_name = dataset.data_name  # dataset name\n",
    "timestamp = time.time()  # run time\n",
    "model_name = config.recommender  # model name\n",
    "model_param = config.summarize()  # return a string of model parameters\n",
    "param_str = f\"{data_name}_{model_name}_{model_param}\"\n",
    "run_id = f\"{param_str[:150]}_{timestamp:.8f}\"\n",
    "\n",
    "log_dir = os.path.join(\"log\", data_name, model_name)  # logger directory\n",
    "logger_name = os.path.join(log_dir, run_id + \".log\")  # full path\n",
    "logger = Logger(logger_name)\n",
    "\n",
    "logger.info(dataset)  # show and write dataset info\n",
    "logger.info(config)  # show and write config info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "\n",
    "Evaluation metrics of `Evaluator` are configurable and can automatically fit both leave-one-out and fold-out data splitting without specific indication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reckit import Evaluator\n",
    "\n",
    "user_train_dict = dataset.train_data.to_user_dict()\n",
    "user_test_dict = dataset.test_data.to_user_dict()\n",
    "evaluator = Evaluator(user_train_dict, user_test_dict,\n",
    "                      metric=config.metric, top_k=config.top_k,\n",
    "                      batch_size=config.test_batch_size,\n",
    "                      num_thread=config.test_thread)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PairwiseSampler\n",
    "\n",
    "`PairwiseSampler` is an encapsulation of `Dataset` to do negative item sampling and construct training instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import PairwiseSampler\n",
    "\n",
    "data_iter = PairwiseSampler(dataset.train_data, num_neg=1,\n",
    "                            batch_size=config[\"batch_size\"], \n",
    "                            shuffle=True, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from util.pytorch import inner_product\n",
    "from util.pytorch import get_initializer\n",
    "\n",
    "\n",
    "class MF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embed_dim):\n",
    "        super(MF, self).__init__()\n",
    "\n",
    "        # user and item embeddings\n",
    "        self.user_embeddings = nn.Embedding(num_users, embed_dim)\n",
    "        self.item_embeddings = nn.Embedding(num_items, embed_dim)\n",
    "\n",
    "        self.item_biases = nn.Embedding(num_items, 1)\n",
    "\n",
    "        # weight initialization\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self, init_method=\"uniform\"):\n",
    "        init = get_initializer(init_method)\n",
    "        zero_init = get_initializer(\"zeros\")\n",
    "        init(self.user_embeddings.weight)\n",
    "        init(self.item_embeddings.weight)\n",
    "        zero_init(self.item_biases.weight)\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_embs = self.user_embeddings(user_ids)\n",
    "        item_embs = self.item_embeddings(item_ids)\n",
    "        item_bias = self.item_biases(item_ids)\n",
    "        ratings = inner_product(user_embs, item_embs) + torch.squeeze(item_bias)\n",
    "        return ratings\n",
    "\n",
    "    def predict(self, user_ids):\n",
    "        user_ids = torch.from_numpy(np.asarray(user_ids)).long().to(self.item_embeddings.weight.device)\n",
    "        user_embs = self.user_embeddings(user_ids)\n",
    "        ratings = torch.matmul(user_embs, self.item_embeddings.weight.T)\n",
    "        ratings += torch.squeeze(self.item_biases.weight)\n",
    "        return ratings.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-07 09:15:48.084: metrics:\tRecall@10   \tRecall@20   \tNDCG@10     \tNDCG@20     \n2020-09-07 09:15:48.582: epoch 0:\t0.10472979  \t0.16936544  \t0.28584769  \t0.27690309  \n2020-09-07 09:15:48.956: epoch 1:\t0.12934996  \t0.19387847  \t0.35781866  \t0.33092606  \n2020-09-07 09:15:49.297: epoch 2:\t0.13976939  \t0.20833862  \t0.37019661  \t0.34513870  \n2020-09-07 09:15:49.636: epoch 3:\t0.14822304  \t0.22378592  \t0.37593025  \t0.35645196  \n2020-09-07 09:15:49.997: epoch 4:\t0.15608349  \t0.23660547  \t0.38383657  \t0.36740601  \n2020-09-07 09:15:50.336: epoch 5:\t0.15860106  \t0.24483863  \t0.39068785  \t0.37650907  \n2020-09-07 09:15:50.690: epoch 6:\t0.16183163  \t0.24846396  \t0.39420494  \t0.38145208  \n2020-09-07 09:15:51.027: epoch 7:\t0.16267784  \t0.25457168  \t0.39702213  \t0.38693318  \n2020-09-07 09:15:51.367: epoch 8:\t0.16336972  \t0.25615507  \t0.39993602  \t0.38893095  \n2020-09-07 09:15:51.729: epoch 9:\t0.16387241  \t0.25937146  \t0.40065935  \t0.39105812  \n2020-09-07 09:15:52.070: epoch 10:\t0.16473994  \t0.25964627  \t0.40418732  \t0.39308029  \n2020-09-07 09:15:52.426: epoch 11:\t0.16472246  \t0.26092732  \t0.40247479  \t0.39385366  \n2020-09-07 09:15:52.770: epoch 12:\t0.16481382  \t0.26524559  \t0.40247950  \t0.39654177  \n2020-09-07 09:15:53.111: epoch 13:\t0.16652797  \t0.26602960  \t0.40526214  \t0.39823607  \n2020-09-07 09:15:53.473: epoch 14:\t0.16701843  \t0.27030599  \t0.40628174  \t0.40132818  \n2020-09-07 09:15:53.818: epoch 15:\t0.16768885  \t0.26832953  \t0.40715772  \t0.40099111  \n2020-09-07 09:15:54.175: epoch 16:\t0.16997333  \t0.26933768  \t0.41231146  \t0.40291974  \n2020-09-07 09:15:54.512: epoch 17:\t0.17077792  \t0.27086172  \t0.41586000  \t0.40475443  \n2020-09-07 09:15:54.856: epoch 18:\t0.17198178  \t0.27352512  \t0.42097318  \t0.40991175  \n2020-09-07 09:15:55.228: epoch 19:\t0.17411010  \t0.27515557  \t0.42212000  \t0.41117421  \ndone\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "from util.pytorch import pairwise_loss, pointwise_loss\n",
    "from util.common import Reduction\n",
    "from util.pytorch import l2_loss\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "mf = MF(dataset.num_users, dataset.num_items, config[\"embedding_size\"]).to(device)\n",
    "mf.reset_parameters(config[\"param_init\"])\n",
    "optimizer = torch.optim.Adam(mf.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "logger.info(evaluator.metrics_info())  # show the metrics information\n",
    "for epoch in range(20):\n",
    "    mf.train()\n",
    "    for bat_users, bat_pos_items, bat_neg_items in data_iter:\n",
    "        bat_users = torch.from_numpy(bat_users).long().to(device)\n",
    "        bat_pos_items = torch.from_numpy(bat_pos_items).long().to(device)\n",
    "        bat_neg_items = torch.from_numpy(bat_neg_items).long().to(device)\n",
    "        yui = mf(bat_users, bat_pos_items)\n",
    "        yuj = mf(bat_users, bat_neg_items)\n",
    "\n",
    "        loss = pairwise_loss(\"bpr\", yui-yuj, reduction=Reduction.SUM)\n",
    "        reg_loss = l2_loss(mf.user_embeddings(bat_users),\n",
    "                           mf.item_embeddings(bat_pos_items),\n",
    "                           mf.item_embeddings(bat_neg_items),\n",
    "                           mf.item_biases(bat_pos_items),\n",
    "                           mf.item_biases(bat_neg_items))\n",
    "        loss += config[\"reg\"] * reg_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    mf.eval()\n",
    "    result = evaluator.evaluate(mf)\n",
    "    logger.info(\"epoch %d:\\t%s\" % (epoch, result))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mnt/Study/NeuRec&TFRec/NeuRec/util/tensorflow/func.py:18: The name tf.initializers.he_normal is deprecated. Please use tf.compat.v1.initializers.he_normal instead.\n\nWARNING:tensorflow:From /mnt/Study/NeuRec&TFRec/NeuRec/util/tensorflow/func.py:19: The name tf.initializers.he_uniform is deprecated. Please use tf.compat.v1.initializers.he_uniform instead.\n\nWARNING:tensorflow:From /home/sun/software/anaconda3/envs/neurec3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "import tensorflow as tf\n",
    "from util.tensorflow import inner_product, l2_loss\n",
    "from util.tensorflow import pairwise_loss\n",
    "from util.tensorflow import get_initializer, get_session\n",
    "\n",
    "\n",
    "class MF(object):\n",
    "    def __init__(self, config, num_users, num_items):\n",
    "        self.emb_size = config[\"embedding_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.reg = config[\"reg\"]\n",
    "        self.epochs = config[\"epochs\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.param_init = config[\"param_init\"]\n",
    "        self.loss_func = config[\"loss_func\"]\n",
    "\n",
    "        self.num_users, self.num_items = num_users, num_items\n",
    "\n",
    "        self._build_model()\n",
    "        self.sess = get_session(config[\"gpu_mem\"])\n",
    "\n",
    "    def _create_variable(self):\n",
    "        self.user_ph = tf.placeholder(tf.int32, [None], name=\"user\")\n",
    "        self.pos_item_ph = tf.placeholder(tf.int32, [None], name=\"pos_item\")\n",
    "        self.neg_item_ph = tf.placeholder(tf.int32, [None], name=\"neg_item\")\n",
    "        self.label_ph = tf.placeholder(tf.float32, [None], name=\"label\")\n",
    "\n",
    "        # embedding layers\n",
    "        init = get_initializer(self.param_init)\n",
    "        zero_init = get_initializer(\"zeros\")\n",
    "        self.user_embeddings = tf.Variable(init([self.num_users, self.emb_size]),\n",
    "                                           name=\"user_embedding\")\n",
    "        self.item_embeddings = tf.Variable(init([self.num_items, self.emb_size]),\n",
    "                                           name=\"item_embedding\")\n",
    "        self.item_biases = tf.Variable(zero_init([self.num_items]), name=\"item_bias\")\n",
    "\n",
    "    def _build_model(self):\n",
    "        self._create_variable()\n",
    "        user_emb = tf.nn.embedding_lookup(self.user_embeddings, self.user_ph)\n",
    "        pos_item_emb = tf.nn.embedding_lookup(self.item_embeddings, self.pos_item_ph)\n",
    "        neg_item_emb = tf.nn.embedding_lookup(self.item_embeddings, self.neg_item_ph)\n",
    "        pos_bias = tf.gather(self.item_biases, self.pos_item_ph)\n",
    "        neg_bias = tf.gather(self.item_biases, self.neg_item_ph)\n",
    "\n",
    "        yi_hat = inner_product(user_emb, pos_item_emb) + pos_bias\n",
    "        yj_hat = inner_product(user_emb, neg_item_emb) + neg_bias\n",
    "\n",
    "        # reg loss\n",
    "        model_loss = pairwise_loss(\"bpr\", yi_hat-yj_hat, reduction=Reduction.SUM)\n",
    "        reg_loss = l2_loss(user_emb, pos_item_emb, pos_bias, neg_item_emb, neg_bias)\n",
    "\n",
    "        final_loss = model_loss + self.reg * reg_loss\n",
    "\n",
    "        self.train_opt = tf.train.AdamOptimizer(self.lr).minimize(final_loss, name=\"train_opt\")\n",
    "\n",
    "        # for evaluation\n",
    "        u_emb = tf.nn.embedding_lookup(self.user_embeddings, self.user_ph)\n",
    "        self.batch_ratings = tf.matmul(u_emb, self.item_embeddings, transpose_b=True) + self.item_biases\n",
    "\n",
    "    def predict(self, users):\n",
    "        all_ratings = self.sess.run(self.batch_ratings, feed_dict={self.user_ph: users})\n",
    "        return all_ratings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mnt/Study/NeuRec&TFRec/NeuRec/util/tensorflow/loss.py:60: The name tf.log_sigmoid is deprecated. Please use tf.math.log_sigmoid instead.\n\nWARNING:tensorflow:From /mnt/Study/NeuRec&TFRec/NeuRec/util/tensorflow/func.py:46: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\nWARNING:tensorflow:From /mnt/Study/NeuRec&TFRec/NeuRec/util/tensorflow/func.py:50: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nWARNING:tensorflow:From /mnt/Study/NeuRec&TFRec/NeuRec/util/tensorflow/func.py:51: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n\n2020-09-07 09:15:56.572: metrics:\tRecall@10   \tRecall@20   \tNDCG@10     \tNDCG@20     \n2020-09-07 09:15:56.883: epoch 0:\t0.09928416  \t0.16243352  \t0.28622663  \t0.27297863  \n2020-09-07 09:15:57.095: epoch 1:\t0.11891628  \t0.17962168  \t0.33301952  \t0.30941290  \n2020-09-07 09:15:57.291: epoch 2:\t0.12585293  \t0.19218649  \t0.34554151  \t0.32398596  \n2020-09-07 09:15:57.499: epoch 3:\t0.13744630  \t0.20872766  \t0.35996604  \t0.34028333  \n2020-09-07 09:15:57.699: epoch 4:\t0.15097035  \t0.22414714  \t0.37924343  \t0.35717216  \n2020-09-07 09:15:57.916: epoch 5:\t0.15545547  \t0.23639758  \t0.38656548  \t0.36843264  \n2020-09-07 09:15:58.153: epoch 6:\t0.16020486  \t0.24717879  \t0.39365819  \t0.37842610  \n2020-09-07 09:15:58.363: epoch 7:\t0.16115277  \t0.25156903  \t0.39415100  \t0.38187385  \n2020-09-07 09:15:58.580: epoch 8:\t0.16128425  \t0.25755256  \t0.39690319  \t0.38882911  \n2020-09-07 09:15:58.797: epoch 9:\t0.16385663  \t0.25982428  \t0.40062752  \t0.39204168  \n2020-09-07 09:15:59.054: epoch 10:\t0.16487202  \t0.26228160  \t0.40310854  \t0.39450154  \n2020-09-07 09:15:59.258: epoch 11:\t0.16588835  \t0.26571751  \t0.40620288  \t0.39732617  \n2020-09-07 09:15:59.471: epoch 12:\t0.16430905  \t0.26638120  \t0.40363842  \t0.39788979  \n2020-09-07 09:15:59.672: epoch 13:\t0.16762115  \t0.27116311  \t0.40970737  \t0.40463054  \n2020-09-07 09:15:59.874: epoch 14:\t0.16779733  \t0.27171010  \t0.41114804  \t0.40507314  \n2020-09-07 09:16:00.083: epoch 15:\t0.16975252  \t0.27272019  \t0.41570434  \t0.40807375  \n2020-09-07 09:16:00.312: epoch 16:\t0.16876660  \t0.27329800  \t0.41576833  \t0.40932202  \n2020-09-07 09:16:00.523: epoch 17:\t0.16988377  \t0.27452454  \t0.41886079  \t0.41247526  \n2020-09-07 09:16:00.718: epoch 18:\t0.17156047  \t0.27668414  \t0.42268953  \t0.41525307  \n2020-09-07 09:16:00.941: epoch 19:\t0.17402816  \t0.27819863  \t0.42799228  \t0.41762254  \ndone\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "num_users, num_items = dataset.num_users, dataset.num_items\n",
    "mf = MF(config, num_users, num_items)\n",
    "logger.info(evaluator.metrics_info())\n",
    "for epoch in range(20):\n",
    "    for bat_users, bat_pos_items, bat_neg_items in data_iter:\n",
    "        feed = {mf.user_ph: bat_users,\n",
    "                mf.pos_item_ph: bat_pos_items,\n",
    "                mf.neg_item_ph: bat_neg_items}\n",
    "        mf.sess.run(mf.train_opt, feed_dict=feed)\n",
    "    result = evaluator.evaluate(mf)\n",
    "    logger.info(\"epoch %d:\\t%s\" % (epoch, result))\n",
    "print(\"done\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2.0,
  "kernelspec": {
   "name": "python361064bitneurec3condadc95a1fcfe8042408bc60cd247b20baa",
   "display_name": "Python 3.6.10 64-bit ('neurec3': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
